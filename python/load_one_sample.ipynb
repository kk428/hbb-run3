{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d557a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f56885c",
   "metadata": {},
   "source": [
    "# Explore parquet files\n",
    "\n",
    "This notebook explores different versions of parquet files to validate the total number of events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5f789",
   "metadata": {},
   "source": [
    "Gabi's old files produced with ptSkimmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bcba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/eos/uscms/store/user/kakrzyza/2024/ZLWmL/pickles\n",
      "All pickle files: []\n",
      "Total sum of weights for all pickles for ZLWmL: 0\n"
     ]
    }
   ],
   "source": [
    "path_to_dir = Path(\"/eos/uscms/store/user/gmachado/bbbb/ptSkimmer/24Aug13_v12/\")\n",
    "year = \"2023\"\n",
    "data_dir = Path(path_to_dir) / year\n",
    "\n",
    "dataset = \"GluGluHto2B_PT-200_M-125\"\n",
    "\n",
    "pickles_path = Path(data_dir / dataset / \"pickles\")\n",
    "print(pickles_path)\n",
    "all_pickles = list(pickles_path.glob(\"*.pkl\"))\n",
    "print(\"All pickle files:\", all_pickles)\n",
    "total_sumw = 0\n",
    "for pickle_file in all_pickles:\n",
    "    with Path(pickle_file).open(\"rb\") as file:\n",
    "        out_dict = pickle.load(file)\n",
    "    sumw = out_dict[year][dataset][\"nevents\"]\n",
    "    print(f\"Sum of weights for {pickle_file.name}: {sumw}\")\n",
    "    total_sumw += sumw\n",
    "print(f\"Total sum of weights for all pickles for {dataset}: {total_sumw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0eadb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/eos/uscms/store/user/kakrzyza/2024/ZLWmL/parquet\n"
     ]
    }
   ],
   "source": [
    "print(Path(data_dir / dataset / \"parquet\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1ca6455",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "No match for FieldRef.Name(('weight', '0')) in __fragment_index: int32\n__batch_index: int32\n__last_in_fragment: bool\n__filename: string",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m             ret_columns.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_columns\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m events_tmp = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m events_tmp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pandas/io/parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pandas/io/parquet.py:274\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    268\u001b[39m     path,\n\u001b[32m    269\u001b[39m     filesystem,\n\u001b[32m    270\u001b[39m     storage_options=storage_options,\n\u001b[32m    271\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m )\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     result = pa_table.to_pandas(**to_pandas_kwargs)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pyarrow/parquet/core.py:1824\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[32m   1813\u001b[39m     dataset = ParquetFile(\n\u001b[32m   1814\u001b[39m         source, read_dictionary=read_dictionary,\n\u001b[32m   1815\u001b[39m         memory_map=memory_map, buffer_size=buffer_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1821\u001b[39m         page_checksum_verification=page_checksum_verification,\n\u001b[32m   1822\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1824\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pyarrow/parquet/core.py:1475\u001b[39m, in \u001b[36mParquetDataset.read\u001b[39m\u001b[34m(self, columns, use_threads, use_pandas_metadata)\u001b[39m\n\u001b[32m   1467\u001b[39m         index_columns = [\n\u001b[32m   1468\u001b[39m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1470\u001b[39m         ]\n\u001b[32m   1471\u001b[39m         columns = (\n\u001b[32m   1472\u001b[39m             \u001b[38;5;28mlist\u001b[39m(columns) + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) - \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[32m   1473\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1475\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1477\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[32m   1481\u001b[39m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:579\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:415\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.scanner\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:3676\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner.from_dataset\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:3589\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner._make_scan_options\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pyarrow/_dataset.pyx:3539\u001b[39m, in \u001b[36mpyarrow._dataset._populate_builder\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/uscms_data/d3/kkrzyzan/conda/envs/myEnv/lib/python3.13/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowInvalid\u001b[39m: No match for FieldRef.Name(('weight', '0')) in __fragment_index: int32\n__batch_index: int32\n__last_in_fragment: bool\n__filename: string"
     ]
    }
   ],
   "source": [
    "# Load the events from the parquet files\n",
    "def format_columns(columns: list):\n",
    "    ret_columns = []\n",
    "    for key, num_columns in columns:\n",
    "        for i in range(num_columns):\n",
    "            ret_columns.append(f\"('{key}', '{i}')\")\n",
    "    return ret_columns\n",
    "\n",
    "\n",
    "events_tmp = pd.read_parquet(\n",
    "    Path(data_dir / dataset / \"parquet\").as_posix(),\n",
    "    columns=format_columns([(\"weight\", 1)]),\n",
    "    filters=None,\n",
    ")\n",
    "events_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd14d43",
   "metadata": {},
   "source": [
    "Exercise loading one sample with new parquets and pickles from the `categorizer.py` processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cecfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"signal-all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the directory structure\n",
    "MAIN_DIR = \"/eos/uscms/store/user/lpchbbrun3/\"\n",
    "dir_name = \"cmantill/25Jun25_v12\"\n",
    "path_to_dir = Path(f\"{MAIN_DIR}/{dir_name}\")\n",
    "\n",
    "year = \"2022EE\"\n",
    "data_dir = Path(path_to_dir) / year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9490f",
   "metadata": {},
   "source": [
    "Let's look at all the possible samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03190c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all datasets in the directory\n",
    "full_dataset_list = [p.name for p in data_dir.iterdir() if p.is_dir()]\n",
    "print(\"Full samples list:\", full_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ffe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters to apply\n",
    "filters = None\n",
    "\n",
    "# Define a single sample\n",
    "# dataset = \"QCD_HT-600to800\"\n",
    "# dataset = \"JetMET_Run2022E\"\n",
    "dataset = \"GluGluHto2B_PT-200_M-125\"\n",
    "\n",
    "# If you want to see what files are available, uncomment the following lines:\n",
    "print(data_dir)\n",
    "print(list(Path(data_dir / dataset / \"parquet\").glob(f\"{region}*.parquet\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one single parquet file\n",
    "events_tmp = pd.read_parquet(\n",
    "    # \"/eos/uscms/store/user/lpchbbrun3/cmantill/25Jun25_v12/2022EE/GluGluHto2B_PT-200_M-125/parquet/signal-all_0.parquet\",\n",
    "    \"/uscms_data/d3/cmantill/hbb/hbb-run3/signal-all_0-1.parquet\",\n",
    "    filters=filters,\n",
    ")\n",
    "events_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85114344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check one column\n",
    "events_tmp[\"Jet2_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6732e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the events from the all parquet files in one folder\n",
    "# Note: Adjust the filters and columns as needed\n",
    "\n",
    "# for the signal-all region, loading all the columns is not recommended since it will take a lot of memory\n",
    "# Instead, we can load only the columns we need\n",
    "load_columns = [\"FatJet0_msd\", \"weight\"]\n",
    "\n",
    "events_tmp = pd.read_parquet(\n",
    "    list(Path(data_dir / dataset / \"parquet\").glob(f\"{region}*.parquet\")),\n",
    "    columns=load_columns,\n",
    "    filters=filters,\n",
    ")\n",
    "events_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3edd3",
   "metadata": {},
   "source": [
    "The following explores also the pickle files, which we need to extract the total number of MC events before a selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open one pickle file to load the output dictionary, this is an example\n",
    "dataset = \"GluGluHto2B_PT-200_M-125\"\n",
    "pickles_path = Path(f\"{data_dir}/{dataset}/pickles/out_0.pkl\")\n",
    "with Path(pickles_path).open(\"rb\") as file:\n",
    "    out_dict_tmp = pickle.load(file)\n",
    "out_dict_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of how to access sum of genweights for a specific sample\n",
    "for key in out_dict_tmp:\n",
    "    print(next(iter(out_dict_tmp[key][\"sumw\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sum for all the pickles\n",
    "dataset = \"QCD_HT-600to800\"\n",
    "# dataset = \"GluGluHto2B_PT-200_M-125\"\n",
    "\n",
    "pickles_path = Path(data_dir / dataset / \"pickles\")\n",
    "all_pickles = list(pickles_path.glob(\"*.pkl\"))\n",
    "print(\"All pickle files:\", all_pickles)\n",
    "total_sumw = 0\n",
    "for pickle_file in all_pickles:\n",
    "    with Path(pickle_file).open(\"rb\") as file:\n",
    "        out_dict = pickle.load(file)\n",
    "    # The sum of weights is stored in the \"sumw\" key\n",
    "    # You can access it like this:\n",
    "    for key in out_dict:\n",
    "        sumw = next(iter(out_dict[key][\"sumw\"].values()))\n",
    "    print(f\"Sum of weights for {pickle_file.name}: {sumw}\")\n",
    "    total_sumw += sumw\n",
    "print(f\"Total sum of weights for all pickles: {total_sumw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the weight column in the events DataFrame already accounts for:\n",
    "# - the weight of each event, e.g. the generator weight, etc\n",
    "# - times the luminosity\n",
    "# - times the cross section of the process\n",
    "events_tmp[\"weight_nonorm\"] = events_tmp[\"weight\"]\n",
    "events_tmp[\"finalWeight\"] = events_tmp[\"weight\"] / total_sumw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75188100",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_tmp[\"finalWeight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e07af",
   "metadata": {},
   "source": [
    "Let's make one dictionary that stores the sum of gen weights for all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b02346",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\n",
    "    \"hbb\": {\"GluGluHto2B_PT-200_M-125\"},\n",
    "    \"QCD\": {\n",
    "        \"QCD_HT-200to400\",\n",
    "        \"QCD_HT-400to600\",\n",
    "        \"QCD_HT-600to800\",\n",
    "        \"QCD_HT-800to1000\",\n",
    "        \"QCD_HT-1000to1200\",\n",
    "        \"QCD_HT-1200to1500\",\n",
    "        \"QCD_HT-1500to2000\",\n",
    "        \"QCD_HT-2000\",\n",
    "    },\n",
    "    \"TT\": {\"TTto2L2Nu\", \"TTto4Q\", \"TTtoLNu2Q\"},\n",
    "    \"data\": {\n",
    "        \"JetMET_Run2022E\",\n",
    "        \"JetMET_Run2022F\",\n",
    "        \"JetMET_Run2022G\",\n",
    "    },\n",
    "    \"SingleTop\": {\n",
    "        \"TBbarQ_t-channel_4FS\",\n",
    "        \"TbarBQ_t-channel_4FS\",\n",
    "        \"TWminusto2L2Nu\",\n",
    "        \"TWminusto4Q\",\n",
    "        \"TWminustoLNu2Q\",\n",
    "        \"TbarWplusto2L2Nu\",\n",
    "        \"TbarWplusto4Q\",\n",
    "        \"TbarWplustoLNu2Q\",\n",
    "    },\n",
    "    \"Diboson\": {\n",
    "        \"WW\",\n",
    "        \"WZ\",\n",
    "        \"ZZ\",\n",
    "    },\n",
    "    \"WJets\": {\n",
    "        \"Wto2Q-3Jets_HT-200to400\",\n",
    "        \"Wto2Q-3Jets_HT-400to600\",\n",
    "        \"Wto2Q-3Jets_HT-600to800\",\n",
    "        \"Wto2Q-3Jets_HT-800\",\n",
    "    },\n",
    "    \"ZJets\": {\n",
    "        \"Zto2Q-4Jets_HT-200to400\",\n",
    "        \"Zto2Q-4Jets_HT-400to600\",\n",
    "        \"Zto2Q-4Jets_HT-600to800\",\n",
    "        \"Zto2Q-4Jets_HT-800\",\n",
    "    },\n",
    "}\n",
    "\n",
    "sum_genweights = {}\n",
    "for sample_name, datasets in samples.items():\n",
    "    for dataset in datasets:\n",
    "        if sample_name != \"data\":\n",
    "            # For datasets that are not data, we need to load the pickles\n",
    "            # Modify the total weight column accounting for the normalization\n",
    "            total_sumw = 0\n",
    "            for pickle_file in list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\")):\n",
    "                with Path(pickle_file).open(\"rb\") as file:\n",
    "                    out_dict = pickle.load(file)\n",
    "                # The sum of weights is stored in the \"sumw\" key\n",
    "                # You can access it like this:\n",
    "                for key in out_dict:\n",
    "                    sumw = next(iter(out_dict[key][\"sumw\"].values()))\n",
    "                total_sumw += sumw\n",
    "\n",
    "            print(f\"Total sum of weights for all pickles: {total_sumw}\")\n",
    "            sum_genweights[dataset] = total_sumw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945a985",
   "metadata": {},
   "source": [
    "Now, try loading parquets for all samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df68b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc72dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to load\n",
    "load_columns = [\n",
    "    \"FatJet0_pt\",\n",
    "    \"FatJet0_msd\",\n",
    "    \"weight\",\n",
    "]\n",
    "\n",
    "# These columns are specific to the signal samples\n",
    "load_columns_signal = [\n",
    "    \"GenHPt\",\n",
    "    #'GenHEta',\n",
    "    #'GenHPhi'\n",
    "    #'GenHMass'\n",
    "    #'GenHChildren',\n",
    "    #'GenbEta',\n",
    "    #'GenbPhi',\n",
    "    #'GenbMass',\n",
    "    #'GenbPt',\n",
    "]\n",
    "\n",
    "# Filters to apply\n",
    "filters = None\n",
    "\n",
    "# Load the events from the parquet files\n",
    "events_dict = {}\n",
    "for sample_name, datasets in samples.items():\n",
    "    events_list = []\n",
    "    for dataset in datasets:\n",
    "        # Uncomment the following lines to see the dataset and files being loaded\n",
    "        # print(f\"Loading dataset: {dataset}\")\n",
    "        # print(f\"Looking in: {data_dir / dataset / 'parquet' / f'{region}*.parquet'}\")\n",
    "        # print(list(Path(data_dir / dataset / \"parquet\").glob(f'{region}*.parquet')))\n",
    "\n",
    "        columns_to_load = load_columns\n",
    "\n",
    "        # Example of how to load additional columns for specific samples\n",
    "        if sample_name == \"hbb\":\n",
    "            columns_to_load = load_columns + load_columns_signal\n",
    "\n",
    "        print(f\"Loading columns: {columns_to_load}\")\n",
    "\n",
    "        events = pd.read_parquet(\n",
    "            list(Path(data_dir / dataset / \"parquet\").glob(f\"{region}*.parquet\")),\n",
    "            filters=filters,\n",
    "            columns=columns_to_load,\n",
    "        )\n",
    "\n",
    "        if \"data\" not in sample_name:\n",
    "            events[\"weight_nonorm\"] = events[\"weight\"]\n",
    "            events[\"finalWeight\"] = events[\"weight\"] / sum_genweights.get(\n",
    "                dataset, 1.0\n",
    "            )  # Default to 1.0 if not found\n",
    "            print(f\"Using sum_genweights for {dataset}: {sum_genweights.get(dataset, 1.0)}\")\n",
    "        else:\n",
    "            # For data, we just keep the weight as is\n",
    "            events[\"weight_nonorm\"] = events[\"weight\"]\n",
    "            events[\"finalWeight\"] = events[\"weight\"]\n",
    "\n",
    "        events_list.append(events)\n",
    "        print(f\"Loaded {dataset: <50}: {len(events)} entries\")\n",
    "\n",
    "    # Combine all DataFrames for the process/sample\n",
    "    if events_list:\n",
    "        events_dict[sample_name] = pd.concat(events_list)\n",
    "    else:\n",
    "        print(f\"No valid events loaded for sample {sample_name}.\", stacklevel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963afd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dict[\"hbb\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5a9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dict[\"data\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9259e64",
   "metadata": {},
   "source": [
    "Let's make a simple histogram of the `FatJet_pt` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcef548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the hist library: https://hist.readthedocs.io/en/latest/\n",
    "import hist\n",
    "\n",
    "# impor matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "\n",
    "# set the style to CMS\n",
    "plt.style.use(hep.style.CMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86262de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# styles\n",
    "DATA_STYLE = {\n",
    "    \"histtype\": \"errorbar\",\n",
    "    \"color\": \"black\",\n",
    "    \"markersize\": 15,\n",
    "    \"elinewidth\": 2,\n",
    "    \"capsize\": 0,\n",
    "}\n",
    "\n",
    "# colors for the histograms\n",
    "colors = {\n",
    "    \"hbb\": \"tab:purple\",\n",
    "    \"QCD\": \"tab:orange\",\n",
    "    \"TT\": \"tab:blue\",\n",
    "    \"ZJets\": \"tab:green\",\n",
    "    \"WJets\": \"tab:red\",\n",
    "    \"SingleTop\": \"tab:cyan\",\n",
    "    \"Diboson\": \"tab:gray\",\n",
    "    \"data\": \"black\",\n",
    "}\n",
    "\n",
    "# order of histograms in stack\n",
    "mc_names = [\"ZJets\", \"WJets\", \"Diboson\", \"SingleTop\", \"TT\", \"QCD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb58061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pt histogram axes, 30 bins from 200 to 900\n",
    "pt_axis = hist.axis.Regular(30, 300, 900, name=\"pt1\", label=r\"Jet $p_{T}$ [GeV]\")\n",
    "msd_axis = hist.axis.Regular(23, 40, 201, name=\"msd1\", label=\"Jet $m_{sd}$\")\n",
    "\n",
    "# Define the sample axis\n",
    "process_axis = hist.axis.StrCategory(\n",
    "    [],  # This will be filled later with sample names\n",
    "    name=\"process_sample\",\n",
    "    label=\"Sample\",\n",
    "    growth=True,  # Allow dynamic growth of categories\n",
    ")\n",
    "\n",
    "# Make two histograms: one normalized and one non-normalized\n",
    "# The normalized histogram will be used for plotting\n",
    "hist_fatjet = hist.Hist(process_axis, pt_axis, msd_axis)\n",
    "hist_fatjet_nonorm = hist.Hist(process_axis, pt_axis, msd_axis)\n",
    "\n",
    "for sample_name, events in events_dict.items():\n",
    "    # Fill the histogram with weight for each sample\n",
    "    hist_fatjet.fill(\n",
    "        sample_name,\n",
    "        events[\"FatJet0_pt\"],\n",
    "        events[\"FatJet0_msd\"],\n",
    "        weight=events[\"finalWeight\"],\n",
    "    )\n",
    "    hist_fatjet_nonorm.fill(\n",
    "        sample_name,\n",
    "        events[\"FatJet0_pt\"],\n",
    "        events[\"FatJet0_msd\"],\n",
    "        weight=events[\"weight_nonorm\"],  # Use non-normalized weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af1727",
   "metadata": {},
   "source": [
    "This is how a single histogram looks like (the 2nd axis represents the process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a5ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fatjet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this selects the TT process\n",
    "# and sums over the FatJet mass\n",
    "hist_fatjet[{\"process_sample\": \"TT\", \"msd1\": sum}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21646e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, but this selects the QCD process\n",
    "hist_fatjet[{\"process_sample\": \"QCD\", \"msd1\": sum}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's sum over the FatJet pt to plot the mass\n",
    "hist_fatjet[{\"process_sample\": \"QCD\", \"pt1\": sum}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "h_tmp = hist_fatjet[{\"msd1\": sum}]\n",
    "# h_tmp = hist_fatjet_nonorm[{\"msd1\": sum}]\n",
    "\n",
    "# manually stack the histograms for each process\n",
    "hep.histplot(\n",
    "    [h_tmp[{\"process_sample\": mc_name}] for mc_name in mc_names],\n",
    "    label=mc_names,\n",
    "    facecolor=[colors[process_name] for process_name in mc_names],\n",
    "    stack=True,\n",
    "    histtype=\"fill\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1e-2, 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c3a8c",
   "metadata": {},
   "source": [
    "Now, let's add data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef45c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the error of the ratio is calculated using the Poisson interval\n",
    "from hist.intervals import poisson_interval\n",
    "\n",
    "# plot the pt\n",
    "# my_data_hist = hist_fatjet[{\"process_sample\": \"data\", \"msd1\": sum}]\n",
    "# my_mc_hists = [hist_fatjet[{\"process_sample\": process_name, \"msd1\":sum}] for process_name in mc_names]\n",
    "# plot the msd\n",
    "my_data_hist = hist_fatjet[{\"process_sample\": \"data\", \"pt1\": sum}]\n",
    "my_mc_hists = [\n",
    "    hist_fatjet[{\"process_sample\": process_name, \"pt1\": sum}] for process_name in mc_names\n",
    "]\n",
    "\n",
    "fig, (ax, rax) = plt.subplots(\n",
    "    2, 1, figsize=(12, 12), gridspec_kw={\"height_ratios\": [3.5, 1], \"hspace\": 0.18}, sharex=True\n",
    ")\n",
    "\n",
    "# plot the data\n",
    "hep.histplot(\n",
    "    my_data_hist,\n",
    "    ax=ax,\n",
    "    histtype=\"errorbar\",\n",
    "    color=\"black\",\n",
    "    label=\"Data\",\n",
    "    capsize=4,\n",
    "    yerr=True,\n",
    "    flow=\"none\",\n",
    ")\n",
    "\n",
    "# plot the MC processes\n",
    "my_mc_hist_sum = sum(my_mc_hists)\n",
    "hep.histplot(\n",
    "    my_mc_hists,\n",
    "    label=mc_names,\n",
    "    facecolor=[colors[process_name] for process_name in mc_names],\n",
    "    stack=True,\n",
    "    histtype=\"fill\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.xaxis.grid(True, which=\"major\")\n",
    "ax.yaxis.grid(True, which=\"major\")\n",
    "\n",
    "# plot the ratio\n",
    "my_ratio = my_data_hist / (my_mc_hist_sum + 1e-5)  # avoid division by zero\n",
    "\n",
    "yerr = np.nan_to_num(\n",
    "    np.abs(poisson_interval(my_data_hist.values()) - my_data_hist.values())\n",
    "    / (my_mc_hist_sum.values() + 1e-5)  # avoid division by zero\n",
    ")\n",
    "hep.histplot(\n",
    "    my_ratio,\n",
    "    ax=rax,\n",
    "    histtype=\"errorbar\",\n",
    "    color=\"black\",\n",
    "    label=\"Data/MC Ratio\",\n",
    "    capsize=4,\n",
    "    yerr=yerr,\n",
    "    flow=\"none\",\n",
    ")\n",
    "rax.xaxis.grid(True, which=\"major\")\n",
    "rax.yaxis.grid(True, which=\"major\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1, 4e7)\n",
    "ax.legend(bbox_to_anchor=(1.03, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5b606",
   "metadata": {},
   "source": [
    "Let's load the equivalent histogram directly from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open one pickle file to load a histogram\n",
    "pickles_path = Path(f\"{data_dir}/GluGluHto2B_PT-200_M-125/pickles/out_0.pkl\")\n",
    "with Path(pickles_path).open(\"rb\") as file:\n",
    "    out_dict_tmp = pickle.load(file)\n",
    "out_dict_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of weights without selections\n",
    "out_dict_tmp[\"2022EE_GluGluHto2B_PT-200_M-125\"][\"sumw\"][\"2022EE_GluGluHto2B_PT-200_M-125\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the histogram\n",
    "out_dict_tmp[\"2022EE_GluGluHto2B_PT-200_M-125\"][\"templates\"][{\"region\": region}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ae3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should compare the normalization of the histogram with the nominal weights (systematic=nominal)\n",
    "out_dict_tmp[\"2022EE_GluGluHto2B_PT-200_M-125\"][\"templates\"][\n",
    "    {\"region\": region, \"systematic\": \"nominal\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba87d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the pickle file above we see that the total sum of weights is stored in the Sum: WeightedSum(value=...) printout\n",
    "\n",
    "# let's verify that we get the same value from the parquet file\n",
    "parquet_path = Path(f\"{data_dir}/GluGluHto2B_PT-200_M-125/parquet/{region}_0.parquet\")\n",
    "events_tmp = pd.read_parquet(parquet_path, columns=[\"FatJet0_pt\", \"FatJet0_msd\", \"weight\"])\n",
    "print(\"total \", events_tmp[\"weight\"].sum())\n",
    "print(\"w msd > 40\", events_tmp.loc[events_tmp[\"FatJet0_msd\"] > 40, \"weight\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b066042",
   "metadata": {},
   "source": [
    "## From histograms\n",
    "\n",
    "Here is an example of how to load the histograms from the pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will store the histograms for each sample\n",
    "allhists_region = {}\n",
    "# this will store the histograms for each sample without scaling by the sum of weights\n",
    "allhists_region_nonorm = {}\n",
    "\n",
    "print(\"region:\", region)\n",
    "# for sample_name in samples:\n",
    "for sample_name in [\"hbb\"]:\n",
    "    allhists_region[sample_name] = None\n",
    "    for dataset in samples[sample_name]:\n",
    "        print(f\"Loading dataset: {dataset}, sample_name: {sample_name}\")\n",
    "        print(list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\")))\n",
    "        for pickle_file in list(Path(data_dir / dataset / \"pickles\").glob(\"*.pkl\")):\n",
    "            print(f\"Processing pickle file: {pickle_file}\")\n",
    "            with Path(pickle_file).open(\"rb\") as file:\n",
    "                out_dict = pickle.load(file)\n",
    "            # there is only one key in the out_dict, so we can iterate over it\n",
    "            for key in out_dict:\n",
    "                # for now just take the nominal histogram\n",
    "                try:\n",
    "                    # we will sum over all the axes to get a single histogram of the msd\n",
    "                    hist_region = out_dict[key][\"templates\"][{\"region\": region}][\n",
    "                        {\n",
    "                            \"systematic\": \"nominal\",\n",
    "                            \"dataset\": key,\n",
    "                            # \"pnet1\": sum,\n",
    "                            # \"mjj\": sum,\n",
    "                            # \"genflavor\": sum,\n",
    "                        }\n",
    "                    ]\n",
    "                    print(f\"Loaded histogram for {key} from {data_dir / dataset }\")\n",
    "                except KeyError:\n",
    "                    print(f\"KeyError for {region} in {data_dir / dataset / 'pickles'}, skipping.\")\n",
    "\n",
    "                sumw = sum_genweights.get(dataset, 1.0)\n",
    "                print(f\"Using sum_genweights for {dataset}: {sumw}\")\n",
    "\n",
    "                # if this is the first histogram, just assign it\n",
    "                # otherwise, add it to the existing histogram\n",
    "                if not allhists_region[sample_name]:\n",
    "                    print(f\"First histogram for {sample_name}, initializing.\")\n",
    "                    allhists_region[sample_name] = hist_region / sumw\n",
    "                    allhists_region_nonorm[sample_name] = hist_region\n",
    "                else:\n",
    "                    print(f\"Adding histogram for {sample_name}.\")\n",
    "                    allhists_region[sample_name] += hist_region / sumw\n",
    "                    allhists_region_nonorm[sample_name] += hist_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fba9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "allhists_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b851f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allhists_region[\"hbb\"][{\"pt1\": sum}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "allhists_region_nonorm[\"hbb\"][{\"pt1\": sum}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a010d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fatjet[{\"process_sample\": \"QCD\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "allhists_region[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, rax) = plt.subplots(\n",
    "    2, 1, figsize=(12, 12), gridspec_kw={\"height_ratios\": [3.5, 1], \"hspace\": 0.18}, sharex=True\n",
    ")\n",
    "\n",
    "my_data_hist = allhists_region[\"data\"][\n",
    "    {\n",
    "        \"pt1\": sum,\n",
    "        \"pnet1\": sum,\n",
    "        \"mjj\": sum,\n",
    "        \"genflavor\": sum,\n",
    "    }\n",
    "]\n",
    "my_mc_hists = [\n",
    "    allhists_region[process_name][\n",
    "        {\n",
    "            \"pt1\": sum,\n",
    "            \"pnet1\": sum,\n",
    "            \"mjj\": sum,\n",
    "            \"genflavor\": sum,\n",
    "        }\n",
    "    ]\n",
    "    for process_name in mc_names\n",
    "]\n",
    "\n",
    "# plot the data\n",
    "hep.histplot(\n",
    "    my_data_hist,\n",
    "    ax=ax,\n",
    "    histtype=\"errorbar\",\n",
    "    color=\"black\",\n",
    "    label=\"Data\",\n",
    "    capsize=4,\n",
    "    yerr=True,\n",
    "    flow=\"none\",\n",
    ")\n",
    "# plot the MC processes\n",
    "my_mc_hist_sum = sum(my_mc_hists)\n",
    "hep.histplot(\n",
    "    my_mc_hists,\n",
    "    label=mc_names,\n",
    "    facecolor=[colors[process_name] for process_name in mc_names],\n",
    "    stack=True,\n",
    "    histtype=\"fill\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.xaxis.grid(True, which=\"major\")\n",
    "ax.yaxis.grid(True, which=\"major\")\n",
    "\n",
    "# plot the ratio...\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylim(1, 4e7)\n",
    "ax.legend(bbox_to_anchor=(1.03, 1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ed3bd",
   "metadata": {},
   "source": [
    "The `allhists_region_nonorm` should be the same as the `hist_nonorm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04577701",
   "metadata": {},
   "outputs": [],
   "source": [
    "allhists_region[\"hbb\"][{\"pt1\": sum, \"pnet1\": sum, \"mjj\": sum, \"genflavor\": sum}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "allhists_region_nonorm[\"hbb\"][{\"pt1\": sum, \"pnet1\": sum, \"mjj\": sum, \"genflavor\": sum}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd68798",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fatjet[{\"pt1\": sum, \"process_sample\": \"hbb\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e994e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "allhists_region[\"hbb\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
